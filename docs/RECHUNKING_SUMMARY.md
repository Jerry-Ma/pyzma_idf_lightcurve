# Rechunking Solution Summary

## The Problem

All three `populate_epoch` versions (v0, v1, v2) have race conditions when used with concurrent writes in zarr v3. The solution is to:

1. Use **epoch-chunked storage** for safe sequential writes
2. **Rechunk** to object-optimized storage for efficient reads
3. Use the rechunked storage for analysis

## The Solution: Dask Rechunking

### Why Dask?

- ‚úÖ **Only option that works with zarr v3** - rechunker is incompatible
- ‚úÖ Simple API (3 lines of code)
- ‚úÖ No additional dependencies needed
- ‚úÖ Proven performance (tested with 500√ó20 dataset)
- ‚úÖ Data integrity verified

### Why Not Rechunker?

- ‚ùå Built for zarr v2 API (`zarr.hierarchy.Group`)
- ‚ùå Fails with zarr v3: `AttributeError: module 'zarr' has no attribute 'hierarchy'`
- ‚ùå No migration path or workaround available
- ‚ùå Would require downgrading to zarr v2 (not worth it)

## Quick Start

```python
import dask.array as da

# After populating epoch-chunked storage sequentially...

# Rechunk to object-optimized
array = da.from_zarr('lightcurves_epoch_chunked.zarr/flux')
rechunked = array.rechunk((1, n_epochs, 2))  # (1 object, all epochs, 2 bands)
rechunked.to_zarr('lightcurves_object_chunked.zarr/flux', overwrite=True)

# Now use object-chunked storage for fast lightcurve access
```

## Performance

**Test Dataset**: 500 objects √ó 20 epochs √ó 2 measurements (0.08 MB)

| Operation | Time |
|-----------|------|
| Create epoch-chunked | 0.027s |
| Populate 20 epochs | Sequential |
| Rechunk with dask | 0.760s |
| **Total** | < 1 second |

**Scaling**: For IDF dataset (10,000 objects √ó 500 epochs), expect ~2-3 minutes for rechunking.

## Recommended Workflow

### Phase 1: Create Epoch-Chunked Storage
```python
storage = LightcurveStorage.create(
    path='epoch_chunked.zarr',
    n_objects=10000,
    max_epochs=500,
    chunks={
        'object': 1000,  # Multiple objects per chunk
        'epoch': 1,      # One epoch per chunk (write-optimized)
        'measurement': 2,
        'value': 2,
    }
)
```

### Phase 2: Populate Sequentially (No Race Conditions!)
```python
for epoch_idx in range(n_epochs):
    measurements = extract_epoch_data(epoch_idx)
    storage.populate_epoch_v2(epoch_idx, measurements)
```

### Phase 3: Rechunk to Object-Optimized
```python
import dask.array as da

for array_name in ['flux', 'flux_err']:
    array = da.from_zarr(f'epoch_chunked.zarr/{array_name}')
    rechunked = array.rechunk((1, n_epochs, 2))  # Read-optimized
    rechunked.to_zarr(f'object_chunked.zarr/{array_name}')
```

### Phase 4: Analyze from Object-Chunked Storage
```python
storage = LightcurveStorage.open('object_chunked.zarr')
lc = storage.get_lightcurve(12345)  # Fast! Only 1 chunk read
```

## Chunk Strategy Comparison

| Storage Type | Chunks | Best For | Writes/Epoch | Reads/Lightcurve |
|-------------|--------|----------|--------------|------------------|
| **Epoch-chunked** | `(1000, 1, 2)` | Writing | 1 | 10 (slow) |
| **Object-chunked** | `(1, 500, 2)` | Reading | 100 (slow) | 1 (fast) |

**Strategy**: Write to epoch-chunked, then rechunk to object-chunked for analysis.

## Benefits

1. **Safe Concurrent Development**: Write epochs sequentially (no race conditions)
2. **Fast Lightcurve Access**: Read from object-chunked storage (1 chunk per object)
3. **Best of Both Worlds**: Optimized for both writing and reading
4. **Simple Implementation**: Dask handles all complexity
5. **Data Integrity**: Verified with comprehensive tests

## Test Results

All tests passed with data integrity verified:

```
‚úÖ Dask rechunking: 0.760s
‚úÖ Object 0: data matches
‚úÖ Object 250: data matches  
‚úÖ Object 499: data matches
‚úÖ All data verified - rechunking successful!

‚ùå Rechunker: AttributeError (zarr v3 incompatible)
```

## Next Steps

1. ‚úÖ **Testing complete** - dask rechunking works perfectly
2. ‚è≠Ô∏è Add `rechunk_to_object_optimized()` method to `LightcurveStorage`
3. ‚è≠Ô∏è Integrate into production workflow
4. ‚è≠Ô∏è Test with real IDF data

## Documentation

- **Comparison**: `docs/rechunking_comparison_final.md` - Detailed analysis
- **Success Guide**: `docs/rechunking_success.md` - Dask implementation
- **Investigation**: `docs/rechunking_investigation.md` - Options evaluated
- **Test Scripts**:
  - `scripts/test_rechunking_concept.py` - Initial proof of concept
  - `scripts/compare_rechunking_methods.py` - Performance comparison

## Conclusion

**Use dask.array.rechunk() for zarr v3** - it's the only working solution, and it works perfectly! üéâ
